# ğŸ¤– LangGraph Demo

A demonstration project showcasing LangGraph integration with Ollama for building intelligent conversation agents. ğŸš€

## ğŸ“‹ Overview

This project contains two different chatbot implementations using LangGraph and Ollama:

1. **Simple Bot** (`simple_bot.py`) - A basic chatbot that responds to user input ğŸ’¬
2. **Agentic Bot** (`agentic_bot.py`) - An intelligent agent that classifies messages and routes them to specialized handlers (therapist or logical agent) ğŸ§ 

## âœ¨ Features

- **LangGraph Integration**: Uses LangGraph for building stateful conversation flows ğŸ”„
- **Ollama Support**: Leverages local Ollama models for language processing ğŸ 
- **Message Classification**: Automatically classifies user messages as emotional or logical ğŸ¯
- **Dual Agent System**: Routes messages to either a therapist agent or logical agent based on content ğŸ¤
- **Interactive CLI**: Command-line interface for real-time conversations ğŸ’»

## ğŸ“ Project Structure

```
langgraph-demo/
â”œâ”€â”€ pyproject.toml          # Project dependencies and metadata
â”œâ”€â”€ README.md               # This file
â”œâ”€â”€ uv.lock                 # Lock file for dependencies
â””â”€â”€ src/
    â”œâ”€â”€ simple_bot.py       # Basic chatbot implementation
    â””â”€â”€ agentic_bot.py      # Advanced agentic chatbot with routing
```

## ğŸ“‹ Prerequisites

- Python 3.13 or higher ğŸ
- [Ollama](https://ollama.ai/) installed and running locally ğŸ¦™
- A compatible Ollama model (e.g., llama2, mistral, etc.) ğŸ¤–

## ğŸš€ Installation

1. **Clone the repository** ğŸ“¥:
   ```bash
   git clone <repository-url>
   cd langgraph-demo
   ```

2. **Install dependencies using uv** (recommended) âš¡:
   ```bash
   uv sync
   ```

   Or using pip ğŸ“¦:
   ```bash
   pip install -e .
   ```

3. **Set up environment variables** ğŸ”§:
   Create a `.env` file in the project root:
   ```env
   OLLAMA_MODEL=gemma3:1b  # or your preferred model
   OLLAMA_URL=http://localhost:11434
   ```

4. **Ensure Ollama is running** ğŸƒ:
   ```bash
   ollama serve
   ```

   And pull your desired model:
   ```bash
   ollama pull llama2
   ```

## ğŸ¯ Usage

### Simple Bot ğŸ¤–

Run the basic chatbot:

```bash
python src/simple_bot.py
```

This will prompt you for a single message and return a response. ğŸ’­

### Agentic Bot ğŸ§ 

Run the advanced chatbot with message classification:

```bash
python src/agentic_bot.py
```

This starts an interactive conversation where:
- Messages are automatically classified as "emotional" or "logical" ğŸ“Š
- Emotional messages are routed to a therapist agent ğŸ’š
- Logical messages are routed to a fact-based assistant ğŸ§®
- Type `exit` or `quit` to end the conversation ğŸ‘‹

## âš™ï¸ How It Works

### Simple Bot Architecture ğŸ—ï¸

1. User inputs a message ğŸ’¬
2. Message is processed by the LLM through LangGraph ğŸ”„
3. Response is generated and displayed âœ¨

### Agentic Bot Architecture ğŸ›ï¸

1. **Classifier Node**: Analyzes the user's message and classifies it as emotional or logical ğŸ”
2. **Router Node**: Determines which agent should handle the message ğŸš¦
3. **Therapist Agent**: Provides empathetic, emotional support responses ğŸ’š
4. **Logical Agent**: Provides fact-based, analytical responses ğŸ“Š

The flow uses LangGraph's conditional edges to route messages appropriately. ğŸ›¤ï¸

## ğŸ“¦ Dependencies

- **langchain[ollama]**: LangChain framework with Ollama integration ğŸ”—
- **langgraph**: Graph-based workflow orchestration ğŸŒ
- **python-dotenv**: Environment variable management ğŸ”§
- **ipykernel**: Jupyter notebook support ğŸ““
- **pydantic**: Data validation and settings management âœ…

## âš™ï¸ Configuration

The project uses environment variables for configuration:

- `OLLAMA_MODEL`: The Ollama model to use (default: specified in .env) ğŸ¤–
- `OLLAMA_URL`: The Ollama server URL (default: http://localhost:11434) ğŸŒ

## ğŸ¤ Contributing

1. Fork the repository ğŸ´
2. Create a feature branch ğŸŒ¿
3. Make your changes âœï¸
4. Add tests if applicable ğŸ§ª
5. Submit a pull request ğŸš€

## ğŸ”’ License

Pablo Pin - devidence.dev Â©

## ğŸ› ï¸ Troubleshooting

### Common Issues âš ï¸

1. **"Connection refused" error**: Ensure Ollama is running (`ollama serve`) ğŸ”Œ
2. **Model not found**: Pull the required model (`ollama pull <model-name>`) ğŸ“¥
3. **Environment variables not loaded**: Verify your `.env` file is in the project root ğŸ“

### Getting Help ğŸ†˜

- Check the [LangGraph documentation](https://langchain-ai.github.io/langgraph/) ğŸ“š
- Visit the [Ollama documentation](https://ollama.ai/docs) ğŸ“–
- Open an issue in this repository ğŸ›

---

*This demo showcases the power of combining LangGraph's workflow orchestration with Ollama's local language models to create intelligent, context-aware conversation agents.* âœ¨ğŸ¤–ğŸ’«
